{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:19:41.032085Z","iopub.status.busy":"2021-12-18T23:19:41.03178Z","iopub.status.idle":"2021-12-18T23:19:41.035856Z","shell.execute_reply":"2021-12-18T23:19:41.035149Z","shell.execute_reply.started":"2021-12-18T23:19:41.032054Z"},"id":"HPx1WEK4upeX","trusted":true},"outputs":[],"source":["filename = 'data/deu.txt'"]},{"cell_type":"markdown","metadata":{"id":"hthaIcoJvBZg"},"source":["### Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:19:41.960927Z","iopub.status.busy":"2021-12-18T23:19:41.960379Z","iopub.status.idle":"2021-12-18T23:19:48.809047Z","shell.execute_reply":"2021-12-18T23:19:48.808076Z","shell.execute_reply.started":"2021-12-18T23:19:41.960888Z"},"id":"j9rJ967gvDoa","trusted":true},"outputs":[],"source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:19:48.811136Z","iopub.status.busy":"2021-12-18T23:19:48.810861Z","iopub.status.idle":"2021-12-18T23:19:48.819529Z","shell.execute_reply":"2021-12-18T23:19:48.818483Z","shell.execute_reply.started":"2021-12-18T23:19:48.811102Z"},"trusted":true},"outputs":[],"source":["tf.__version__"]},{"cell_type":"markdown","metadata":{"id":"3dJz4OpVvY_9"},"source":["### Parsing the data\n","\n","Each line in our dataset contains an English sentence and its corresposnding German Translation. English sentences are seperated by a tab and German sentences are seperated by a new line.\n","The English sentence is the source sentences and the German sentence is the target sentences. We want to append the token \"[start]\" and \"[end]\" at the start and the end of the each german sentence respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:19:48.821689Z","iopub.status.busy":"2021-12-18T23:19:48.821342Z","iopub.status.idle":"2021-12-18T23:19:50.569715Z","shell.execute_reply":"2021-12-18T23:19:50.568625Z","shell.execute_reply.started":"2021-12-18T23:19:48.821643Z"},"id":"Xv5smVbfvfAW","trusted":true},"outputs":[],"source":["with open(filename) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","\n","text_pairs = []\n","for line in lines:\n","    eng, deu, _ = line.split(\"\\t\")\n","    deu = \"[start] \" + deu + \" [end]\"\n","    text_pairs.append((eng, deu))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:19:50.572004Z","iopub.status.busy":"2021-12-18T23:19:50.571676Z","iopub.status.idle":"2021-12-18T23:19:50.578707Z","shell.execute_reply":"2021-12-18T23:19:50.577937Z","shell.execute_reply.started":"2021-12-18T23:19:50.571963Z"},"id":"Oot7QwWuyH4g","outputId":"aba1a710-ab9e-4f22-c4af-520bf682cedd","trusted":true},"outputs":[],"source":["for _ in range(5):\n","    print(random.choice(text_pairs))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:19:50.580342Z","iopub.status.busy":"2021-12-18T23:19:50.579934Z","iopub.status.idle":"2021-12-18T23:19:50.899684Z","shell.execute_reply":"2021-12-18T23:19:50.898689Z","shell.execute_reply.started":"2021-12-18T23:19:50.580288Z"},"id":"JF-c9vUWyRN9","outputId":"e70fe8ad-be9c-47e6-df28-6905709bc91d","trusted":true},"outputs":[],"source":["# Let's split the data into training and validation sets.\n","\n","random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples : ]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"]},{"cell_type":"markdown","metadata":{"id":"SlSU498nzMn3"},"source":["### Vectorizing the text data\n","\n","Now we have to vectorize the text data. We will use the TextVectorization layer to vectorize the text data (one for english and one for german).."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:19:54.237307Z","iopub.status.busy":"2021-12-18T23:19:54.236666Z","iopub.status.idle":"2021-12-18T23:19:54.245499Z","shell.execute_reply":"2021-12-18T23:19:54.24443Z","shell.execute_reply.started":"2021-12-18T23:19:54.237249Z"},"trusted":true},"outputs":[],"source":["strip_chars = string.punctuation\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","@tf.keras.utils.register_keras_serializable()\n","def custom_standardization(input_string):\n","    text = tf.strings.lower(input_string)\n","    \n","    text = tf.strings.regex_replace(text, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-17T14:24:54.28326Z","iopub.status.busy":"2021-12-17T14:24:54.28243Z","iopub.status.idle":"2021-12-17T14:25:29.467953Z","shell.execute_reply":"2021-12-17T14:25:29.467191Z","shell.execute_reply.started":"2021-12-17T14:24:54.283223Z"},"id":"DHKTe5QXzT-l","trusted":true},"outputs":[],"source":["eng_vocab_size = 16000\n","deu_vocab_size = 35000\n","sequence_length = 30\n","batch_size = 64\n","\n","strip_chars = string.punctuation\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","@tf.keras.utils.register_keras_serializable()\n","def custom_standardization(input_string):\n","    text = tf.strings.lower(input_string)\n","    \n","    text = tf.strings.regex_replace(text, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","    return text\n","\n","eng_vector = TextVectorization(\n","    max_tokens=eng_vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","\n","deu_vector = TextVectorization(\n","    max_tokens=deu_vocab_size, output_mode=\"int\",\n","    output_sequence_length = sequence_length+1,\n","    standardize=custom_standardization,\n",")\n","\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_deu_texts = [pair[1] for pair in train_pairs]\n","eng_vector.adapt(train_eng_texts)\n","deu_vector.adapt(train_deu_texts)"]},{"cell_type":"markdown","metadata":{},"source":["### Saving the tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-17T14:25:29.470111Z","iopub.status.busy":"2021-12-17T14:25:29.469856Z","iopub.status.idle":"2021-12-17T14:25:30.348275Z","shell.execute_reply":"2021-12-17T14:25:30.347543Z","shell.execute_reply.started":"2021-12-17T14:25:29.470078Z"},"trusted":true},"outputs":[],"source":["eng_vector_model = tf.keras.models.Sequential()\n","eng_vector_model.add(tf.keras.Input(shape=(None, ), dtype=tf.string))\n","eng_vector_model.add(eng_vector)\n","eng_vector_model.summary()\n","\n","eng_vector_model.save('eng_vector_layer')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-17T14:25:30.350757Z","iopub.status.busy":"2021-12-17T14:25:30.350529Z","iopub.status.idle":"2021-12-17T14:25:30.519262Z","shell.execute_reply":"2021-12-17T14:25:30.518495Z","shell.execute_reply.started":"2021-12-17T14:25:30.350732Z"},"trusted":true},"outputs":[],"source":["print(eng_vector_model.predict(['hello son']))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:05.765782Z","iopub.status.busy":"2021-12-18T23:20:05.765225Z","iopub.status.idle":"2021-12-18T23:20:06.732069Z","shell.execute_reply":"2021-12-18T23:20:06.731136Z","shell.execute_reply.started":"2021-12-18T23:20:05.765745Z"},"trusted":true},"outputs":[],"source":["eng_token = tf.keras.models.load_model('models/eng_vector_layer')\n","eng_token.predict(['hello son']).to_tensor(default_value=0, shape=[None, 30])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-17T14:25:45.424975Z","iopub.status.busy":"2021-12-17T14:25:45.424435Z","iopub.status.idle":"2021-12-17T14:25:46.000843Z","shell.execute_reply":"2021-12-17T14:25:46.000095Z","shell.execute_reply.started":"2021-12-17T14:25:45.424932Z"},"trusted":true},"outputs":[],"source":["deu_vector_model = tf.keras.models.Sequential()\n","deu_vector_model.add(tf.keras.Input(shape=(None, ), dtype=tf.string))\n","deu_vector_model.add(deu_vector)\n","deu_vector_model.summary()\n","\n","deu_vector_model.save('deu_vector_layer')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:12.101872Z","iopub.status.busy":"2021-12-18T23:20:12.101507Z","iopub.status.idle":"2021-12-18T23:20:12.519562Z","shell.execute_reply":"2021-12-18T23:20:12.51841Z","shell.execute_reply.started":"2021-12-18T23:20:12.101822Z"},"trusted":true},"outputs":[],"source":["deu_token = tf.keras.models.load_model('models/deu_vector_later')"]},{"cell_type":"markdown","metadata":{},"source":["We'll save the tokenizer for the later use.\n","\n","Now, Let's create our dataset using `tf.data.Dataset` API.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond) using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple (`inputs`, `targets`), where:\n","* `inputs` consist of the source sentence and the target words 0 to N.\n","* `target` is the target sentence offset by one step: it provides the next words in the target sentence -- what the model will try to predict."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T13:08:05.755382Z","iopub.status.busy":"2021-12-18T13:08:05.754527Z","iopub.status.idle":"2021-12-18T13:08:07.644241Z","shell.execute_reply":"2021-12-18T13:08:07.643242Z","shell.execute_reply.started":"2021-12-18T13:08:05.755324Z"},"id":"3lSDMVxf8_m1","trusted":true},"outputs":[],"source":["def format_dataset(eng, deu):\n","    eng = eng_vector(eng)\n","    deu = deu_vector(deu)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": deu[:, :-1]}, deu[:, 1:])\n","\n","def make_dataset(pairs):\n","    eng_texts, deu_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    deu_texts = list(deu_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, deu_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-17T14:26:00.217015Z","iopub.status.busy":"2021-12-17T14:26:00.216762Z","iopub.status.idle":"2021-12-17T14:26:01.33681Z","shell.execute_reply":"2021-12-17T14:26:01.334492Z","shell.execute_reply.started":"2021-12-17T14:26:00.216979Z"},"id":"cDm05q-W_bZj","outputId":"c086a4fd-c254-4192-b95a-78a92da848e1","trusted":true},"outputs":[],"source":["for inputs, targets in train_ds.take(1):\n","    print(f'input[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"target.shape: {targets.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"OfdXBxr3AEuS"},"source":["### Build the model\n","\n","Our sequence to sequence Transformer consist of a `TransformerEncoder` and a `TransformerDecoder` chained together. To make the model aware of word order, we use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`, which will produce a new representation of it. This new representation will be then passed to the `TransformerDecoder`, together with the target sequence so far (target word 0 to N). The `TransformerDecoder` will then seek to predict the next word in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is casual masking (see the method `get_casual_attention_maski()` on the `TransformerDecoder`). The `TransformerDecoder` see the entire sequences at once, and thus we must make sure that it only uses information from target tokens 0 to N when predicting the token N+1 (otherwise, it could be use information from the future, which would result in a model that cannot be used at inference time)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:23.446529Z","iopub.status.busy":"2021-12-18T23:20:23.446184Z","iopub.status.idle":"2021-12-18T23:20:23.461349Z","shell.execute_reply":"2021-12-18T23:20:23.46053Z","shell.execute_reply.started":"2021-12-18T23:20:23.446489Z"},"id":"MyeJiD9VAPOT","trusted":true},"outputs":[],"source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads):\n","        super(TransformerEncoder, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation='relu'), layers.Dense(embed_dim)]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","    \n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        \n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","    \n","    def get_config(self):\n","        return {\n","            \"embed_dim\" : self.embed_dim,\n","            \"dense_dim\" : self.dense_dim,\n","            \"num_heads\" : self.num_heads,\n","               }\n","    \n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:24.14042Z","iopub.status.busy":"2021-12-18T23:20:24.13991Z","iopub.status.idle":"2021-12-18T23:20:24.150635Z","shell.execute_reply":"2021-12-18T23:20:24.14954Z","shell.execute_reply.started":"2021-12-18T23:20:24.140365Z"},"id":"xkeXXHtNCIUm","trusted":true},"outputs":[],"source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim):\n","        super(PositionalEmbedding, self).__init__()\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.positional_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.positional_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","    \n","    def get_config(self):\n","        return {\n","            \"sequence_length\" : self.sequence_length,\n","            \"vocab_size\" : self.vocab_size,\n","            \"embed_dim\" : self.embed_dim,\n","        }\n","    \n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:24.81795Z","iopub.status.busy":"2021-12-18T23:20:24.817636Z","iopub.status.idle":"2021-12-18T23:20:24.835686Z","shell.execute_reply":"2021-12-18T23:20:24.834944Z","shell.execute_reply.started":"2021-12-18T23:20:24.817914Z"},"id":"vG1GecOCFJp5","trusted":true},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads):\n","        super(TransformerDecoder, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation='relu'), layers.Dense(embed_dim)]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","    \n","    def call(self, inputs, encoder_outputs, mask=None):\n","        casual_mask = self.get_casual_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n","            padding_mask = tf.minimum(padding_mask, casual_mask)\n","        \n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=casual_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","        \n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_casual_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype='int32')\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","             axis=0,\n","        )\n","\n","        return tf.tile(mask, mult)\n","    \n","    def get_config(self):\n","        return {\n","            \"embed_dim\" : self.embed_dim,\n","            \"latent_dim\" : self.latent_dim,\n","            \"num_heads\" : self.num_heads\n","        }\n","    \n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)"]},{"cell_type":"markdown","metadata":{"id":"aZuAX9B4j3bQ"},"source":["### Assemble the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-17T14:26:30.692017Z","iopub.status.busy":"2021-12-17T14:26:30.691362Z","iopub.status.idle":"2021-12-17T14:26:31.543234Z","shell.execute_reply":"2021-12-17T14:26:31.54244Z","shell.execute_reply.started":"2021-12-17T14:26:30.691976Z"},"id":"mGZRCWlYljgP","trusted":true},"outputs":[],"source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None, ), dtype='int64', name='encoder_inputs')\n","x = PositionalEmbedding(sequence_length, eng_vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None, ), dtype='int64', name='decoder_inputs')\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name='decoder_state_inputs')\n","x = PositionalEmbedding(sequence_length, deu_vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(deu_vocab_size, activation='softmax')(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name='transformer'\n",")"]},{"cell_type":"markdown","metadata":{"id":"zcT_rXN0wPnL"},"source":["### Training our model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-17T17:47:19.225001Z","iopub.status.busy":"2021-12-17T17:47:19.224321Z","iopub.status.idle":"2021-12-17T20:45:57.217548Z","shell.execute_reply":"2021-12-17T20:45:57.216764Z","shell.execute_reply.started":"2021-12-17T17:47:19.224962Z"},"id":"HCZaG5TQqwfX","outputId":"8760d3de-3603-4637-f69a-64422b737244","trusted":true},"outputs":[],"source":["epochs = 30\n","\n","transformer.summary()\n","transformer.compile(\n","    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","history = transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T11:32:13.386955Z","iopub.status.busy":"2021-12-14T11:32:13.38674Z","iopub.status.idle":"2021-12-14T11:32:13.392114Z","shell.execute_reply":"2021-12-14T11:32:13.391295Z","shell.execute_reply.started":"2021-12-14T11:32:13.386929Z"},"trusted":true},"outputs":[],"source":["# transformer.save_weights()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-17T20:55:28.674172Z","iopub.status.busy":"2021-12-17T20:55:28.673464Z","iopub.status.idle":"2021-12-17T20:55:29.157253Z","shell.execute_reply":"2021-12-17T20:55:29.156572Z","shell.execute_reply.started":"2021-12-17T20:55:28.674132Z"},"id":"UBXhpMbOoM6u","trusted":true},"outputs":[],"source":["transformer.save('seq2seq-transformer_full_vocab.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-14T11:32:21.314094Z","iopub.status.busy":"2021-12-14T11:32:21.313867Z","iopub.status.idle":"2021-12-14T11:32:21.317288Z","shell.execute_reply":"2021-12-14T11:32:21.316501Z","shell.execute_reply.started":"2021-12-14T11:32:21.31406Z"},"trusted":true},"outputs":[],"source":["# del transformer"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:31.416064Z","iopub.status.busy":"2021-12-18T23:20:31.415089Z","iopub.status.idle":"2021-12-18T23:20:37.766213Z","shell.execute_reply":"2021-12-18T23:20:37.76514Z","shell.execute_reply.started":"2021-12-18T23:20:31.416005Z"},"trusted":true},"outputs":[],"source":["load_model = tf.keras.models.load_model('models/seq2seq-transformer_full_vocab.h5', custom_objects={\n","    \"TransformerEncoder\" : TransformerEncoder,\n","    \"PositionalEmbedding\" : PositionalEmbedding,\n","    \"TransformerDecoder\" : TransformerDecoder\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:47.495963Z","iopub.status.busy":"2021-12-18T23:20:47.495513Z","iopub.status.idle":"2021-12-18T23:20:47.499895Z","shell.execute_reply":"2021-12-18T23:20:47.498863Z","shell.execute_reply.started":"2021-12-18T23:20:47.495928Z"},"trusted":true},"outputs":[],"source":["# load_model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Decoding test sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:48.779206Z","iopub.status.busy":"2021-12-18T23:20:48.778879Z","iopub.status.idle":"2021-12-18T23:20:48.898139Z","shell.execute_reply":"2021-12-18T23:20:48.897115Z","shell.execute_reply.started":"2021-12-18T23:20:48.77917Z"},"trusted":true},"outputs":[],"source":["len(deu_token.layers[0].get_vocabulary())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:49.806232Z","iopub.status.busy":"2021-12-18T23:20:49.805936Z","iopub.status.idle":"2021-12-18T23:20:49.81102Z","shell.execute_reply":"2021-12-18T23:20:49.810291Z","shell.execute_reply.started":"2021-12-18T23:20:49.806203Z"},"trusted":true},"outputs":[],"source":["deu_token_layer = deu_token.layers[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-15T22:58:37.471418Z","iopub.status.busy":"2021-12-15T22:58:37.470891Z","iopub.status.idle":"2021-12-15T22:58:37.519793Z","shell.execute_reply":"2021-12-15T22:58:37.519136Z","shell.execute_reply.started":"2021-12-15T22:58:37.471378Z"},"trusted":true},"outputs":[],"source":["deu_token_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:54.120192Z","iopub.status.busy":"2021-12-18T23:20:54.119261Z","iopub.status.idle":"2021-12-18T23:20:54.242285Z","shell.execute_reply":"2021-12-18T23:20:54.241556Z","shell.execute_reply.started":"2021-12-18T23:20:54.120143Z"},"trusted":true},"outputs":[],"source":["deu_vocab = deu_token.layers[0].get_vocabulary()\n","deu_index_lookup = dict(zip(range(len(deu_vocab)), deu_vocab))\n","max_decode_sentence_length = 30\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_token.predict([input_sentence]).to_tensor(default_value=0, shape=[None, max_decode_sentence_length])\n","    decode_sentence = \"[start]\"\n","    for i in range(max_decode_sentence_length):\n","        tokenized_target_sentence = deu_token.predict([decode_sentence]).to_tensor(default_value=0, shape=[None, max_decode_sentence_length+1])[:, :-1]\n","        predictions = load_model([tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = deu_index_lookup[sampled_token_index]\n","        decode_sentence += \" \" + sampled_token\n","        \n","        if sampled_token == \"[end]\":\n","            break\n","    return decode_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-18T23:20:54.67688Z","iopub.status.busy":"2021-12-18T23:20:54.676259Z","iopub.status.idle":"2021-12-18T23:21:07.266501Z","shell.execute_reply":"2021-12-18T23:21:07.265483Z","shell.execute_reply.started":"2021-12-18T23:20:54.676838Z"},"trusted":true},"outputs":[],"source":["test_eng_texts = [pair[0] for pair in test_pairs]\n","test_deu_texts = [pair[1] for pair in test_pairs]\n","for _ in range(10):\n","    input_sentence, deu_sentence = random.choice(test_pairs)\n","    translated = decode_sequence(input_sentence)\n","    \n","    print(f\" \\\n","    English sentence: \\n  {input_sentence}\\n\\n---\\n\\n \\\n","    German sentence: \\n  {deu_sentence}\\n\\n---\\n\\n \\\n","    Translated sentence: \\n  {translated}\\n\\n------------------\\n\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
